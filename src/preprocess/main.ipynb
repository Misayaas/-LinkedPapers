{
 "cells": [
  {
   "cell_type": "code",
   "id": "0738edcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T10:58:34.163061Z",
     "start_time": "2024-11-24T10:58:31.805611Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as path\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "\n",
    "dataset_dir = \"~/Projects/BigData/hw3\"\n",
    "# Read abstract, category, year of each paper\n",
    "papers = pd.read_csv(path.join(dataset_dir, 'papers.csv.gz'), compression='gzip')\n",
    "\n",
    "# Read the embedding vector of each paper\n",
    "feats = pd.read_csv(path.join(dataset_dir, 'feats.csv.gz'), compression='gzip', header=None).values.astype(np.float32)\n",
    "\n",
    "# Read the citation relations between papers\n",
    "edges = pd.read_csv(path.join(dataset_dir, 'edges.csv.gz'), compression='gzip', header=None).values.T.astype(np.int32)\n",
    "citer, citee = edges\n",
    "\n",
    "# 可以读出title,abstract,category,year\n",
    "print(papers[\"title\"])\n",
    "print(feats)\n",
    "print(edges)"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\HuangMy/Projects/BigData/hw3\\\\papers.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m dataset_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m~/Projects/BigData/hw3\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Read abstract, category, year of each paper\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m papers \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpapers.csv.gz\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgzip\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Read the embedding vector of each paper\u001B[39;00m\n\u001B[0;32m     13\u001B[0m feats \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(path\u001B[38;5;241m.\u001B[39mjoin(dataset_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeats.csv.gz\u001B[39m\u001B[38;5;124m'\u001B[39m), compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgzip\u001B[39m\u001B[38;5;124m'\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\ml_lib\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\ml_lib\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\ml_lib\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\ml_lib\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\ml_lib\\lib\\site-packages\\pandas\\io\\common.py:765\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    761\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgzip\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    762\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    763\u001B[0m         \u001B[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001B[39;00m\n\u001B[0;32m    764\u001B[0m         \u001B[38;5;66;03m# \"GzipFile\", variable has type \"Union[str, BaseBuffer]\")\u001B[39;00m\n\u001B[1;32m--> 765\u001B[0m         handle \u001B[38;5;241m=\u001B[39m gzip\u001B[38;5;241m.\u001B[39mGzipFile(  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[0;32m    766\u001B[0m             filename\u001B[38;5;241m=\u001B[39mhandle,\n\u001B[0;32m    767\u001B[0m             mode\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m    768\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcompression_args,\n\u001B[0;32m    769\u001B[0m         )\n\u001B[0;32m    770\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    771\u001B[0m         handle \u001B[38;5;241m=\u001B[39m gzip\u001B[38;5;241m.\u001B[39mGzipFile(\n\u001B[0;32m    772\u001B[0m             \u001B[38;5;66;03m# No overload variant of \"GzipFile\" matches argument types\u001B[39;00m\n\u001B[0;32m    773\u001B[0m             \u001B[38;5;66;03m# \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    776\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcompression_args,\n\u001B[0;32m    777\u001B[0m         )\n",
      "File \u001B[1;32mF:\\Anaconda\\envs\\ml_lib\\lib\\gzip.py:174\u001B[0m, in \u001B[0;36mGzipFile.__init__\u001B[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001B[0m\n\u001B[0;32m    172\u001B[0m     mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fileobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 174\u001B[0m     fileobj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmyfileobj \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(fileobj, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\HuangMy/Projects/BigData/hw3\\\\papers.csv.gz'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f354fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分\n",
    "train_data = papers[papers['year'] <= 2017]\n",
    "val_data = papers[papers['year'] == 2018]\n",
    "test_data = papers[papers['year'] >= 2019]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# 提取特征和标签\n",
    "X_train = feats[train_data.index]\n",
    "y_train_str = train_data['category'].values\n",
    "y_train = le.fit_transform(y_train_str)\n",
    "\n",
    "X_val = feats[val_data.index]\n",
    "y_val_str = val_data['category']\n",
    "y_val = le.fit_transform(y_val_str)\n",
    "\n",
    "X_test = feats[test_data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac3daaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    f1_scores = []\n",
    "    for label in unique_labels:\n",
    "        tp = np.sum((y_true == label) & (y_pred == label))\n",
    "        fp = np.sum((y_true != label) & (y_pred == label))\n",
    "        fn = np.sum((y_true == label) & (y_pred != label))\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "def KNN_predict(x_train, y_train, k, X):\n",
    "    y_pred = []\n",
    "    print(\"len:\", len(X))\n",
    "    i = 0\n",
    "    for x in X:\n",
    "        distances = np.linalg.norm(x_train - x, axis=1)\n",
    "        nearest_neighbors = np.argsort(distances)[:k]\n",
    "        nearest_labels = y_train[nearest_neighbors]\n",
    "        y_pred.append(np.argmax(np.bincount(nearest_labels)))    \n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Progress:{i}/{len(X)}\")\n",
    "        i += 1\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b28558b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 29799\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "Accuracy: 0.4670290949360717\n",
      "F1 mean: 0.26861034238993187\n"
     ]
    }
   ],
   "source": [
    "y_pred = KNN_predict(X_train, y_train, 3, X_val)\n",
    "        \n",
    "accuracy_mean = accuracy_score(y_val, y_pred)\n",
    "f1_mean = f1_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy_mean}\")\n",
    "print(f\"F1 mean: {f1_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "468f063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 48603\n",
      "Progress:0/48603\n",
      "Progress:1000/48603\n",
      "Progress:2000/48603\n",
      "Progress:3000/48603\n",
      "Progress:4000/48603\n",
      "Progress:5000/48603\n",
      "Progress:6000/48603\n",
      "Progress:7000/48603\n",
      "Progress:8000/48603\n",
      "Progress:9000/48603\n",
      "Progress:10000/48603\n",
      "Progress:11000/48603\n",
      "Progress:12000/48603\n",
      "Progress:13000/48603\n",
      "Progress:14000/48603\n",
      "Progress:15000/48603\n",
      "Progress:16000/48603\n",
      "Progress:17000/48603\n",
      "Progress:18000/48603\n",
      "Progress:19000/48603\n",
      "Progress:20000/48603\n",
      "Progress:21000/48603\n",
      "Progress:22000/48603\n",
      "Progress:23000/48603\n",
      "Progress:24000/48603\n",
      "Progress:25000/48603\n",
      "Progress:26000/48603\n",
      "Progress:27000/48603\n",
      "Progress:28000/48603\n",
      "Progress:29000/48603\n",
      "Progress:30000/48603\n",
      "Progress:31000/48603\n",
      "Progress:32000/48603\n",
      "Progress:33000/48603\n",
      "Progress:34000/48603\n",
      "Progress:35000/48603\n",
      "Progress:36000/48603\n",
      "Progress:37000/48603\n",
      "Progress:38000/48603\n",
      "Progress:39000/48603\n",
      "Progress:40000/48603\n",
      "Progress:41000/48603\n",
      "Progress:42000/48603\n",
      "Progress:43000/48603\n",
      "Progress:44000/48603\n",
      "Progress:45000/48603\n",
      "Progress:46000/48603\n",
      "Progress:47000/48603\n",
      "Progress:48000/48603\n"
     ]
    }
   ],
   "source": [
    "y_test_encoded = KNN_predict(X_train, y_train, 3, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb54d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs.AI' 'cs.CV' 'cs.CV' ... 'cs.CL' 'cs.LG' 'cs.MM']\n"
     ]
    }
   ],
   "source": [
    "le.fit(y_train_str)\n",
    "y_test = le.inverse_transform(y_test_encoded)\n",
    "print(y_test)\n",
    "papers_predicted = papers.copy()\n",
    "papers_predicted.loc[papers['year'] >= 2019, 'category'] = y_test\n",
    "papers_predicted.to_csv(path.join(dataset_dir, 'paper_predicted.csv.gz'), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32e9e17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>Factored Contextual Policy Search with Bayesia...</td>\n",
       "      <td>cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Simultaneous Merging Multiple Grid Maps Using ...</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Reconstruction of Hidden Representation for Ro...</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>A Look at the Time Delays in Cvss Vulnerabilit...</td>\n",
       "      <td>cs.CY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>Common Tangents of Two Disjoint Polygons in Li...</td>\n",
       "      <td>cs.CG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169338</th>\n",
       "      <td>Sentinet Detecting Localized Universal Attacks...</td>\n",
       "      <td>cs.CR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169339</th>\n",
       "      <td>Interpretable Mtl From Heterogeneous Domains U...</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169340</th>\n",
       "      <td>Learning Compositional Rules via Neural Progra...</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169341</th>\n",
       "      <td>Certified Defenses for Adversarial Patches</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169342</th>\n",
       "      <td>Fauras a Proxy Based Framework for Ensuring th...</td>\n",
       "      <td>cs.MM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48603 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title category\n",
       "346     Factored Contextual Policy Search with Bayesia...    cs.AI\n",
       "398     Simultaneous Merging Multiple Grid Maps Using ...    cs.CV\n",
       "451     Reconstruction of Hidden Representation for Ro...    cs.CV\n",
       "480     A Look at the Time Delays in Cvss Vulnerabilit...    cs.CY\n",
       "488     Common Tangents of Two Disjoint Polygons in Li...    cs.CG\n",
       "...                                                   ...      ...\n",
       "169338  Sentinet Detecting Localized Universal Attacks...    cs.CR\n",
       "169339  Interpretable Mtl From Heterogeneous Domains U...    cs.CV\n",
       "169340  Learning Compositional Rules via Neural Progra...    cs.CL\n",
       "169341         Certified Defenses for Adversarial Patches    cs.LG\n",
       "169342  Fauras a Proxy Based Framework for Ensuring th...    cs.MM\n",
       "\n",
       "[48603 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_predicted[papers['year'] >= 2019][['title', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11671986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
